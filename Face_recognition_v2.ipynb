{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPytH+0NSSGFz7BMg5sHfhV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oscarB1nar10/face_recognition/blob/main/Face_recognition_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "c6qcF0TBrp6a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.applications import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect to Google drive"
      ],
      "metadata": {
        "id": "ZsL7rB5XsLzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AzExKEjr-Xu",
        "outputId": "0701ef7f-cbfa-4fd8-d1f6-21760e490b12"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the pretrained model"
      ],
      "metadata": {
        "id": "zMcxoNkntCES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the InceptionV3 model with pre-trained weights\n",
        "# include_top = false, the top of the model, which is responsable for the actual calssification is not loaded.\n",
        "# This is typically done when we want to use the model for feature extraction in a transfer learning scenario.\n",
        "base_model = InceptionV3(include_top=False, weights='imagenet', pooling='avg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VyXp9F1tHCR",
        "outputId": "2546b318-1593-450d-a8be-d44171ea8feb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87910968/87910968 [==============================] - 3s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess the images"
      ],
      "metadata": {
        "id": "JK3bfnIdo9nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_image(image, target_size=(160, 160)):\n",
        "    resized_image = cv2.resize(image, target_size)\n",
        "    # Convert the image colors from BGR to RGB\n",
        "    resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)\n",
        "    normalized_image = resized_image / 255.0\n",
        "    return normalized_image"
      ],
      "metadata": {
        "id": "wm4XMoyqpGNn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and preprocess the images that we are going to use as database images of encodings and labels"
      ],
      "metadata": {
        "id": "8s3hYfC_siVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess images\n",
        "def load_and_preprocess_images(dataset_path):\n",
        "  image_paths = [] # List to store the image paths\n",
        "  labels = [] # List to store the labels corresponding to each image\n",
        "  encodings = [] # List to store the generated encodings\n",
        "\n",
        "  # Loop through the dataset folder\n",
        "  for label in os.listdir(dataset_path):\n",
        "    label_path = os.path.join(dataset_path, label)\n",
        "    if os.path.isdir(label_path):\n",
        "      # Loop through the images in the subfolder\n",
        "      for img_name in os.listdir(label_path):\n",
        "        img_path = os.path.join(label_path, img_name)\n",
        "        if img_path.endswith('.jpg') or img_path.endswith('.png'):\n",
        "          image_paths.append(img_path)\n",
        "          labels.append(label)\n",
        "\n",
        "  # Loop through the image paths and generate encodings\n",
        "  for img_path in image_paths:\n",
        "    img = cv2.imread(img_path) # Read the image from the file path\n",
        "    preprocessed_img = normalize_image(img)\n",
        "    # Add an extra dimension to the image array to make it a batch of one image\n",
        "    # The model expects input in the format (batch_size, height, width, channels)\n",
        "    image_batch = np.expand_dims(preprocessed_img, axis=0)\n",
        "    encodings.append(normalize(base_model.predict(image_batch)))\n",
        "\n",
        "  return image_paths, labels, np.array(encodings)"
      ],
      "metadata": {
        "id": "gYuk5Mq2szMW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the images we want to generate the encodings\n",
        "dataset_path = '/content/drive/MyDrive/face_images/dataset_embeddings/'\n",
        "# Generate encodings for the dataset images\n",
        "image_paths_dataset, labels, dataset_encodings = load_and_preprocess_images(dataset_path)\n",
        "\n",
        "print(f\"dataset_encodings: {dataset_encodings}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zv_CGJivs3ig",
        "outputId": "00ce723e-683a-4f87-908c-2cf0e2e350cd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 99ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 143ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 104ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 139ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 145ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 93ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 142ms/step\n",
            "1/1 [==============================] - 0s 161ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 132ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "1/1 [==============================] - 0s 97ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 136ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 144ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 102ms/step\n",
            "1/1 [==============================] - 0s 91ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 86ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "1/1 [==============================] - 0s 83ms/step\n",
            "1/1 [==============================] - 0s 82ms/step\n",
            "1/1 [==============================] - 0s 96ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 89ms/step\n",
            "1/1 [==============================] - 0s 90ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 146ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 141ms/step\n",
            "1/1 [==============================] - 0s 84ms/step\n",
            "dataset_encodings: [[[0.02463774 0.00540078 0.00528919 ... 0.0016229  0.02033683 0.01174251]]\n",
            "\n",
            " [[0.0081503  0.02689975 0.00161638 ... 0.01613967 0.00532081 0.01586023]]\n",
            "\n",
            " [[0.00221358 0.00983824 0.0057775  ... 0.00389638 0.02416536 0.0210307 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.01782076 0.00353921 0.         ... 0.01039482 0.00392762 0.00398203]]\n",
            "\n",
            " [[0.00520398 0.00842023 0.00317693 ... 0.00382666 0.         0.00943276]]\n",
            "\n",
            " [[0.04431464 0.0282218  0.00721014 ... 0.00333184 0.         0.0347878 ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validate the model accuracy based on L2 distance between the database encodings and the test encodings"
      ],
      "metadata": {
        "id": "WHN3G8NTtVjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_model(validation_encodings, validation_labels, threshold):\n",
        "  correct_predictions = 0\n",
        "\n",
        "  for i in range(len(validation_encodings)):\n",
        "    # Calculate distances with the dataset encodings\n",
        "    distances = []\n",
        "    #min_dis = 100\n",
        "    for emb in dataset_encodings:\n",
        "      distance = np.linalg.norm(tf.subtract(validation_encodings[i], emb))\n",
        "      # if distance < min_dis:\n",
        "      #   min_dis = distance\n",
        "      distances.append(distance)\n",
        "\n",
        "    # Get the index of the closest encoding and it's label\n",
        "    min_index = np.argmin(distances)\n",
        "    predicted_label = labels[min_index]\n",
        "\n",
        "    print(f\"predicted_label: {predicted_label}, validation_labels[i]: {validation_labels[i]}\")\n",
        "\n",
        "    print(f\"distances[min_index]: {distances[min_index]}, threshold: {threshold}\")\n",
        "\n",
        "    # Check if the prediction is correct\n",
        "    if distances[min_index] <= threshold and predicted_label == validation_labels[i]:\n",
        "      correct_predictions += 1\n",
        "      print(f\"correct_predictions: {correct_predictions}\")\n",
        "\n",
        "  accurary = correct_predictions / len(validation_encodings)\n",
        "\n",
        "  return accurary"
      ],
      "metadata": {
        "id": "RjavBM5Bt5st"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the images we want to generate the encoding\n",
        "validation_path = '/content/drive/MyDrive/face_images/test_images/test_images_formatted/'\n",
        "# Generate encoding for the test images\n",
        "image_paths_validation, validation_labels, validation_encoding = load_and_preprocess_images(validation_path)\n",
        "\n",
        "print(f\"validation_encoding: {validation_encoding}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5WAleRgt8qG",
        "outputId": "c587a431-b6c0-47c0-be8b-6cf7503f8d03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 88ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "1/1 [==============================] - 0s 92ms/step\n",
            "1/1 [==============================] - 0s 87ms/step\n",
            "validation_encoding: [[[0.00717166 0.01723029 0.00242884 ... 0.         0.00205334 0.02068376]]\n",
            "\n",
            " [[0.00043814 0.00093427 0.00912452 ... 0.0654472  0.00024958 0.02449374]]\n",
            "\n",
            " [[0.01744594 0.01521931 0.00328817 ... 0.0418946  0.00875952 0.03257682]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.02048094 0.01981575 0.         ... 0.0085188  0.04169054 0.02208139]]\n",
            "\n",
            " [[0.02138359 0.01594866 0.         ... 0.04295041 0.02152747 0.00302541]]\n",
            "\n",
            " [[0.02433598 0.01250073 0.01261056 ... 0.00968251 0.05111663 0.02832301]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy"
      ],
      "metadata": {
        "id": "KKCtoQxUuIJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.7\n",
        "accuracy = validate_model(validation_encoding, validation_labels, threshold)\n",
        "print(f\"Validation accuracy: {accuracy * 100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQqB8QgHuK3l",
        "outputId": "838d9bc0-1f1d-489f-ef38-b1073318814e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted_label: Oscar, validation_labels[i]: Oscar\n",
            "distances[min_index]: 0.5040819048881531, threshold: 0.7\n",
            "correct_predictions: 1\n",
            "predicted_label: Gladys, validation_labels[i]: Gladys\n",
            "distances[min_index]: 0.646894097328186, threshold: 0.7\n",
            "correct_predictions: 2\n",
            "predicted_label: Alejandro, validation_labels[i]: Juan Jose\n",
            "distances[min_index]: 0.7093230485916138, threshold: 0.7\n",
            "predicted_label: Juan Jose, validation_labels[i]: Juan Jose\n",
            "distances[min_index]: 0.5822619199752808, threshold: 0.7\n",
            "correct_predictions: 3\n",
            "predicted_label: Alejandro, validation_labels[i]: Alejandro\n",
            "distances[min_index]: 0.38698285818099976, threshold: 0.7\n",
            "correct_predictions: 4\n",
            "predicted_label: Oscar, validation_labels[i]: Guillermo\n",
            "distances[min_index]: 0.6380272507667542, threshold: 0.7\n",
            "predicted_label: Dianna, validation_labels[i]: Dianna\n",
            "distances[min_index]: 0.3461225926876068, threshold: 0.7\n",
            "correct_predictions: 5\n",
            "Validation accuracy: 71.42857142857143%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUeHUezwuLft"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}